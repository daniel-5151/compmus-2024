---
title: "Portfolio for Computational Musicology"
author: "DaniÃ«l Drucker"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(plotly)
library(knitr)
library(kableExtra)
library(compmus)
library(cowplot)
library(tidymodels)
library(ggdendro)
library(ranger)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  
```

```{r load_playlists, include=FALSE}
playlist_beatles <- get_playlist_audio_features("", "18HKG7EtGQyvhL7t5lFXw6")
playlist_nirvana <- get_playlist_audio_features("", "0G935Lqu0vrEVoPNZNFrqP")
playlist_coldplay <- get_playlist_audio_features("", "1j08FQSS1hD36ldvdVXP3W") 

playlist_full <-
  bind_rows(
    playlist_beatles |> mutate(artist_name = "The Beatles"),
    playlist_nirvana |> mutate(artist_name = "Nirvana"),
    playlist_coldplay |> mutate(artist_name = "Coldplay")
  ) |>
  add_audio_analysis()
```

### Introduction
For the last few years my most listened to genre in my Spotify Wrapped has been Rock Music. Not surprising, since a large part of my personal playlists contain this genre. There is, however, a lot of subgenres within Rock. After it's quick rise of popularity in the 1960's, there have been a lot of different developments and movements. This is also the case in the music that I listen to, where there is a wide range of different subgenres. This got me thinking about what the differences between rock music in different time periods might be.  The main focus of this portfolio will be to uncover these differences within the limits of the Spotify API. We will do so by looking at a few different topics for different tracks. Examples of these will include track-level features, chroma features, different chords used and tempo.  

At first the plan was to look at rock as a whole for this comparison, but I soon found out that this would make the comparisons way to brought with all the different. This is why I narrowed the corpus down to three bands that each was very influential in their time: The Beatles in the 60's, Nirvana in the 80's/90's and Coldplay in the modern era. For each band I made a playlist based on the ones provided by Spotify. These include the most popular tracks. I then removed all the live tracks, since those can intervene with this research. The playlists can be found in the links below: 

- The Beatles: https://open.spotify.com/playlist/18HKG7EtGQyvhL7t5lFXw6?si=943a7297af884c30

- Nirvana: https://open.spotify.com/playlist/0G935Lqu0vrEVoPNZNFrqP?si=d99122f1223f407e

- Coldplay: https://open.spotify.com/playlist/1j08FQSS1hD36ldvdVXP3W?si=b0ec1bff23c644e1


Below are some interesting stats about the corpus for each band:
```{r introduction_table}
artist_stats <- playlist_full %>%
  group_by(artist_name) %>%
  summarise(num_entries = n(),
            total_duration = sum(track.duration_ms) / (1000 * 60),
            min_release_year = min(track.album.release_date),
            max_release_year = max(track.album.release_date))

artist_stats <- rename(artist_stats, "Band Name" = artist_name, 
                       "Number of Tracks" = num_entries,
                       "Total Duration (min)" = total_duration,
                       "First Album Release" = min_release_year,
                       "Last Album Release" = max_release_year)

kable(artist_stats) %>%
  kable_styling(full_width = FALSE, position = "left")
```

***
The Beatles:
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/18HKG7EtGQyvhL7t5lFXw6?utm_source=generator&theme=0" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

Nirvana:
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/0G935Lqu0vrEVoPNZNFrqP?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

Coldplay:
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/1j08FQSS1hD36ldvdVXP3W?utm_source=generator&theme=0" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### Track-Level Features: Valence and Energy
```{r track-features-1}
plot_valence <- playlist_full |>
  ggplot(aes(x = artist_name, y = valence, fill = artist_name, alpha = 0.8)) +
  geom_boxplot() +
  scale_y_continuous(     
    limits = c(0, 1),
    breaks = c(0, 0.25, 0.5, 0.75, 1.0),
    minor_breaks = NULL
  ) +
  theme_light() +
  theme(
    legend.position = "none",
    panel.grid.major.x = element_blank()
  ) +
  labs(
    x = "Band",
    y = "Valence",
    title = "Left: Valence Distribution, Right: Energy Distribution"
  )

plot_energy <- playlist_full |>
  ggplot(aes(x = artist_name, y = energy, fill = artist_name, alpha = 0.8)) +
  geom_boxplot() +
  scale_y_continuous(     
    limits = c(0, 1),
    breaks = c(0, 0.25, 0.5, 0.75, 1.0),
    minor_breaks = NULL
  ) + 
  theme_light() +
  theme(
    legend.position = "none",
    panel.grid.major.x = element_blank()
  ) +
  labs(
    x = "Band",
    y = "Energy",
    title = "Left: Valence Distribution, Right: Energy Distribution"
  )

subplot(ggplotly(plot_valence), ggplotly(plot_energy), nrows = 1)
```

***
The two plots on the left show the distribution of valence (the positiveness of a song) and energy (the intensity of a song) per band.

On the valence graph on the left, we can see that The Beatles have the most "positive" songs, while Coldplay has the most "negative". These two distributions are not surprising and were somewhat expected beforehand. The Beatles have a more lively and upbeat feel, while Coldplay has a lot of more negative songs. Nirvana seems to fall in between the two, which I found interesting. This is because a lot of their songs contain negative subjects, such as anger. What is also interesting to seem is that the songs get more negative over time. However, just looking at things three bands is not enough to conclude this for all of rock music.

When looking at the energy graph on the right, Nirvana is the outlier. They have by far the highest level of energy per track with a median of 0.83. This is not surprising, since rock songs of this time period (especially in "grunge rock") are known for their high levels of intensity. Coldplay follows with a median of 0.63, while The Beatles have a median intensity of 0.43. I also found the results for The Beatles not surprising, since their tracks usually play more slow.

### Chromagram and Cepstrogram

```{r}
outlier <- "11LmqTE2naFULdEP94AUBa"

chroma <-
  get_tidy_audio_analysis(outlier) |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

plot1a <-
  chroma |>
    mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
    compmus_gather_chroma() |>
    ggplot(
      aes(
        x = start + duration / 2,
        width = duration,
        y = pitch_class,
        fill = value
      )
    ) +
    geom_tile() +
    labs(x = "Time (s)",
         y = NULL,
         fill = "Magnitude",
         title = "Chromogram (Heart Shaped Box - Nirvana)") +
    theme_minimal() +
    scale_fill_viridis_c()


chepstro <-
  get_tidy_audio_analysis(outlier) |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"
      )
  )

plot1b <-
  chepstro |>
    compmus_gather_timbre() |>
    ggplot(
      aes(
        x = start + duration / 2,
        width = duration,
        y = basis,
        fill = value
      )
    ) +
    geom_tile() +
    labs(
      x = "Time (s)",
      y = NULL,
      fill = "Magnitude",
      title = "Chepstrogram (Heart Shaped Box - Nirvana)") +
    scale_fill_viridis_c() +
    theme_classic()

plot_grid(plot1a, plot1b, ncol = 1)
```

***

### Self-Simularity Matrices
```{r}
max_popular_beatles <- playlist_beatles[which.max(playlist_beatles$track.popularity), ]
max_popular_nirvana <- playlist_nirvana[which.max(playlist_nirvana$track.popularity), ]
max_popular_coldplay <- playlist_coldplay[which.max(playlist_coldplay$track.popularity), ]

max_popular_nirvana_uri <- max_popular_nirvana$track.uri
max_popular_coldplay_uri <- max_popular_coldplay$track.uri

track1 <-
  get_tidy_audio_analysis("4CeeEOM32jQcH3eN9Q2dGj") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"
      )
  )

track2 <-
  get_tidy_audio_analysis("3AJwUDP919kvQ9QcozQPxg") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"
      )
  )

plot2a <-
  track1 |>
  compmus_self_similarity(pitches, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Chroma (Smells Like Teen Spirit)")

plot2b <-
  track1 |>
  compmus_self_similarity(timbre, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Timbre (Smells Like Teen Spirit)")

plot2c <-
  track2 |>
  compmus_self_similarity(pitches, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Chroma (Yellow)")

plot2d <-
  track2 |>
  compmus_self_similarity(timbre, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Timbre (Yellow)")

plot_grid(plot2a, plot2b, plot2c, plot2d, ncol = 2)

```

***
hallo

### keygram / chordogram

### Track-Level Features: Tempo and Loudness
```{r track-features-2}
playlist_full |>
  mutate(
    sections =
      map(
        sections,                                   
        summarise_at,
        vars(tempo, loudness, duration),            
        list(section_mean = mean, section_sd = sd)
      )
  ) |>
  unnest(sections) |>
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = artist_name,
      alpha = loudness
    )
  ) +
  geom_point(size = 4) +
  geom_rug() +
  theme_light() +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    title = "Variation in Tempo and Loudness",
    colour = "Band Name",
    alpha = "Loudness (dBFS)"
  )


```

***
Now lets have a look at two other important track-level features: tempo and loudness/volume. The plot on the left shows these features for each individual track in the corpus on the sections level. When looking at the alpha of each track, there does not seem to be a big difference in the loudness between tracks. Only The Beatles do have some tracks with a low volume.

The tempo in this graph yields some interesting results. When it comes to the mean tempoin BPM, Nirvana seems to be overall the highest. Coldplay and The Beatles have around the same median tempo for their tracks. The biggest differences, however, are notable with the variation of different used tempo's throughout each track. Most of the tracks of all three bands have a low variation in tempo. Nirvana has again the highest, followed by The Beatles and then Coldplay. All bands also have noticable outliers, with Coldplay having the most. Letâs have a closer look at some of those outliers in the next chapter. 

### A closer Look at Tempo

```{r tempgrams}
beatles_love <- get_tidy_audio_analysis("6KqiVlOLfDzJViJVAPym1p")
nirvana_marigold <- get_tidy_audio_analysis("40VSbBSYbPN10vJYeZq4tm")

tempogram_1 <-
  beatles_love |>
    tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
    ggplot(aes(x = time, y = bpm, fill = power)) +
    geom_raster() +
    scale_fill_viridis_c(guide = "none") +
    labs(title = "All You Need is Love - The Beatles",
        x = "Time (s)",
        y = "Tempo (BPM)") +
    theme_classic()

tempogram_2 <- 
  nirvana_marigold |>
    tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
    ggplot(aes(x = time, y = bpm, fill = power)) +
    geom_raster() +
    scale_fill_viridis_c(guide = "none") +
    labs(title = "Marigold - Nirvana",
        x = "Time (s)",
        y = "Tempo (BPM)") +
    theme_classic()

plot_grid(tempogram_1, tempogram_2, ncol = 1)
```

***
As seen in the previous chapter, there are a few tracks with high variations of tempo. In this chapter we will have a look at some of these. We will not look at the most expreme outliers, but at those in-between 

One of those outliers is "All You Need Is Love" from The Beatles, with a tempo standard deviation of 7.5. The top plot on the left shows the fourier tempogram from this track. Throughout the track, the tempo seems so slowly build up. There is also a lot of strong tempo-harmonics at play. Especially the at 3:1 the tempo envelope. A point of interest is around the 200 second mark, where for a brief moment tempo estimation becomes really hard

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6KqiVlOLfDzJViJVAPym1p?utm_source=generator&theme=0" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

The second outlier is "Marigold" from Nirvana. It has around the variation in tempo of "All You Need Is Love", but a higher overall tempo. This track also slowly build up the tempo throughout the track. A notable difference here is the lack of strong tempo-harmonics. There is only a strong harmonic at two times the tempo envelope, but it is not as strong as the top graph. There is also a weaker harmonic at around 3:1 the tempo envelope. But with this track, there is no big time periods where tempo estimation gets hard.

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/40VSbBSYbPN10vJYeZq4tm?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### What features distinguise the bands the most?
```{r classication-pre-processing, include=FALSE}
band_features <-
  playlist_full |>
  mutate(
    playlist = factor(artist_name),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

band_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = band_features
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())

band_cv <- band_features |> vfold_cv(5)
```

```{r feature-ranking}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
band_forest <- 
  workflow() |> 
  add_recipe(band_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    band_cv, 
    control = control_resamples(save_pred = TRUE)
  )

workflow() |> 
  add_recipe(band_recipe) |> 
  add_model(forest_model) |> 
  fit(band_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature Ranking in Importance", x = NULL, y = "Importance")
```

***
We have now seen a lot of different visualizations that differentiate the three bands? But what features are the most important differences between the bands? To answer this question, a random forest decision tree was trained. This algorithm gives a ranking of how important each feature is for classification. The ranking for the three bands can be seen on this plot.

According to this ranking, there is a big difference between a few features. One of those is valence, which we have already seen in the first chapter. No big surprise that it ranks high, since we have already seen there is a big difference in valence between the bands. Two other important features that we have not seen before is the duration of the tracks and acousticness.

Two other big differences can be seen in the spectral envelop. This is most notable with the c05 and c06 partials. There does not seem to be a big distinction in the different chroma's used.

### Classifying Independant Tracks


### Conclusion