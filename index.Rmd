---
title: "Portfolio for Computational Musicology"
author: "Daniël Drucker"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(plotly)
library(knitr)
library(kableExtra)
library(compmus)
library(cowplot)
library(tidymodels)
library(ggdendro)
library(ranger)

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  
```

```{r load_playlists, include=FALSE}
playlist_beatles <- get_playlist_audio_features("", "18HKG7EtGQyvhL7t5lFXw6")
playlist_nirvana <- get_playlist_audio_features("", "0G935Lqu0vrEVoPNZNFrqP")
playlist_coldplay <- get_playlist_audio_features("", "1j08FQSS1hD36ldvdVXP3W") 

playlist_full <-
  bind_rows(
    playlist_beatles |> mutate(artist_name = "The Beatles"),
    playlist_nirvana |> mutate(artist_name = "Nirvana"),
    playlist_coldplay |> mutate(artist_name = "Coldplay")
  ) |>
  add_audio_analysis()
```

### Introduction
For the last few years my most listened to genre in my Spotify Wrapped has been Rock Music. Not surprising, since a large part of my personal playlists contain this genre. There is, however, a lot of subgenres within Rock. After it's quick rise of popularity in the 1960's, there have been a lot of different developments and movements. This is also the case in the music that I listen to, where there is a wide range of different subgenres. This got me thinking about what the differences between rock music in different time periods might be.  The main focus of this portfolio will be to uncover these differences within the limits of the Spotify API. We will do so by looking at a few different topics for different tracks. Examples of these will include track-level features, chroma features, different chords used and tempo.  

At first the plan was to look at rock as a whole for this comparison, but I soon found out that this would make the comparisons way to brought with all the different. This is why I narrowed the corpus down to three bands that each was very influential in their time: The Beatles in the 60's, Nirvana in the 80's/90's and Coldplay in the modern era. For each band I made a playlist based on the ones provided by Spotify. These include the most popular tracks. I then removed all the live tracks, since those can intervene with this research. The playlists can be found in the links below: 

- The Beatles: https://open.spotify.com/playlist/18HKG7EtGQyvhL7t5lFXw6?si=943a7297af884c30

- Nirvana: https://open.spotify.com/playlist/0G935Lqu0vrEVoPNZNFrqP?si=d99122f1223f407e

- Coldplay: https://open.spotify.com/playlist/1j08FQSS1hD36ldvdVXP3W?si=b0ec1bff23c644e1


Below are some interesting stats about the corpus for each band:
```{r introduction_table}
artist_stats <- playlist_full %>%
  group_by(artist_name) %>%
  summarise(num_entries = n(),
            total_duration = sum(track.duration_ms) / (1000 * 60),
            min_release_year = min(track.album.release_date),
            max_release_year = max(track.album.release_date))

artist_stats <- rename(artist_stats, "Band Name" = artist_name, 
                       "Number of Tracks" = num_entries,
                       "Total Duration (min)" = total_duration,
                       "First Album Release" = min_release_year,
                       "Last Album Release" = max_release_year)

kable(artist_stats) %>%
  kable_styling(full_width = FALSE, position = "left")
```

***
The Beatles:
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/18HKG7EtGQyvhL7t5lFXw6?utm_source=generator&theme=0" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

Nirvana:
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/0G935Lqu0vrEVoPNZNFrqP?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

Coldplay:
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/1j08FQSS1hD36ldvdVXP3W?utm_source=generator&theme=0" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### Track-Level Features: Valence and Energy
```{r track-features-1}
plot_valence <- playlist_full |>
  ggplot(aes(x = artist_name, y = valence, fill = artist_name, alpha = 0.8)) +
  geom_boxplot() +
  scale_y_continuous(     
    limits = c(0, 1),
    breaks = c(0, 0.25, 0.5, 0.75, 1.0),
    minor_breaks = NULL
  ) +
  theme_light() +
  theme(
    legend.position = "none",
    panel.grid.major.x = element_blank()
  ) +
  labs(
    x = "Band",
    y = "Valence",
    title = "Left: Valence Distribution, Right: Energy Distribution"
  )

plot_energy <- playlist_full |>
  ggplot(aes(x = artist_name, y = energy, fill = artist_name, alpha = 0.8)) +
  geom_boxplot() +
  scale_y_continuous(     
    limits = c(0, 1),
    breaks = c(0, 0.25, 0.5, 0.75, 1.0),
    minor_breaks = NULL
  ) + 
  theme_light() +
  theme(
    legend.position = "none",
    panel.grid.major.x = element_blank()
  ) +
  labs(
    x = "Band",
    y = "Energy",
    title = "Left: Valence Distribution, Right: Energy Distribution"
  )

subplot(ggplotly(plot_valence), ggplotly(plot_energy), nrows = 1)
```

***
The two plots on the left show the distribution of valence (the positiveness of a song) and energy (the intensity of a song) per band.

On the valence graph on the left, we can see that The Beatles have the most "positive" songs, while Coldplay has the most "negative". These two distributions are not surprising and were somewhat expected beforehand. The Beatles have a more lively and upbeat feel, while Coldplay has a lot of more negative songs. Nirvana seems to fall in between the two, which I found interesting. This is because a lot of their songs contain negative subjects, such as anger. What is also interesting to seem is that the songs get more negative over time. However, just looking at things three bands is not enough to conclude this for all of rock music.

When looking at the energy graph on the right, Nirvana is the outlier. They have by far the highest level of energy per track with a median of 0.83. This is not surprising, since rock songs of this time period (especially in "grunge rock") are known for their high levels of intensity. Coldplay follows with a median of 0.63, while The Beatles have a median intensity of 0.43. I also found the results for The Beatles not surprising, since their tracks usually play more slow.

### Chromagram and Cepstrogram

```{r}
outlier <- "11LmqTE2naFULdEP94AUBa"

chroma <-
  get_tidy_audio_analysis(outlier) |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

plot1a <-
  chroma |>
    mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
    compmus_gather_chroma() |>
    ggplot(
      aes(
        x = start + duration / 2,
        width = duration,
        y = pitch_class,
        fill = value
      )
    ) +
    geom_tile() +
    labs(x = "Time (s)",
         y = NULL,
         fill = "Magnitude",
         title = "Chromogram (Heart Shaped Box - Nirvana)") +
    theme_minimal() +
    scale_fill_viridis_c()


chepstro <-
  get_tidy_audio_analysis(outlier) |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"
      )
  )

plot1b <-
  chepstro |>
    compmus_gather_timbre() |>
    ggplot(
      aes(
        x = start + duration / 2,
        width = duration,
        y = basis,
        fill = value
      )
    ) +
    geom_tile() +
    labs(
      x = "Time (s)",
      y = NULL,
      fill = "Magnitude",
      title = "Chepstrogram (Heart Shaped Box - Nirvana)") +
    scale_fill_viridis_c() +
    theme_classic()

plot_grid(plot1a, plot1b, ncol = 1)
```

***

### Valence Explained By Tonal Analyses
```{r key-template, include=FALSE}
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r keygram-1}
ob_la_di <-
  get_tidy_audio_analysis("1gFNm7cXfG1vSMcxPpSxec") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

ob_la_di |> 
  compmus_match_pitch_template(
    key_templates,       
    method = "angular",
    norm = "euclidean"    
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "Keygram: Ob-La-Di, Ob-La-Da - The Beatles",x = "Time (s)", y = "Keys")
```

***
In the first chapter, we have seen big differences of valence between each band. The tracks of the Beatles seem to overall have a happy mood, while tracks of Coldplay seem to be more somber. A big contributor of the level of valence within a track is the tone, which is influenced by the different keys. In particular, we should focus on the major and minor keys to explain valance. Major chords tend to be bright, uplifting and joyful and thus having a positive impact on valence. Minor chords on the other hand are experienced as dark and somber, resulting in a lower valence score.  

The plot on the left shows "Ob-La-Di, Ob-La-Da" by The Beatles. This track has one of the highest levels of valence of The Beatles and also of the entire corpus. The distances are represented as angular distances while using euclidean normalization. Throughout the track, the different keys are pretty blurry. We can, however, still make up the most used keys. Most of those are major keys, such as B major, E major and G major. There is not a lot of prominent minor keys. The only two are G# minor and E minor. The prominence of major keys gives a good explanation of the high valence score.

On the next page, we will have a look at a keygram from a Coldplay track. The goal is to see there is a difference is the tone that explains the lower valence of their tracks.

### Tonal Analyses, Continued
```{r keygram-2}
fix_you <-
  get_tidy_audio_analysis("7LVHVU3tWfcxj5aiPFEW4Q") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )


fix_you |> 
  compmus_match_pitch_template(
    key_templates,        
    method = "angular",  
    norm = "euclidean"
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  geom_vline(xintercept = 156, colour = "red") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "Keygram: Fix You - Coldplay",x = "Time (s)", y = "Keys")
```

***
On the previous page, it was mentioned that Coldplay song seem to be more somber than the other two bands. On the left plot is a keygram from "Fix You", one of Coldplay's more somber songs. This track is be roughly divided into two parts: the first part is experienced as really sad and somber, while the second part is a bit more bright and upbeat. This divide happens at 2:36 and is annotated with the red line.

This keygram uses the same normalization and distance measure. As can be seen, there is a lot more spillage of keys and it is not really clear what keys are the most prominent. There does seem to be a good balance between minor and major keys in the first part of the track, which can already result in a more somber tone than "Ob-La-Di, Ob-La-Da". On the second part of the song, it becomes even more impossible to distinguish the different keys. But also here it seems like there is a good balance between minor and major keys.

Because of the high spillage on this keygram, there is not a definitive conclusion of whether the chords used by Coldplay give it a sad character.

### Structure Using Self-Simularity Matrices
```{r self-simularity}
track1 <-
  get_tidy_audio_analysis("4CeeEOM32jQcH3eN9Q2dGj") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"
      )
  )



ssm1 <-
  track1 |>
  compmus_self_similarity(timbre, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Smells Like Teen Spirit")

track2 <-
  get_tidy_audio_analysis("3AJwUDP919kvQ9QcozQPxg") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"
      )
  )

ssm2 <-
  track2 |>
  compmus_self_similarity(timbre, "cosine") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Yellow")

plot_grid(ssm1, ssm2, ncol = 2)

```

***
The plots in this chapter show the structure of two of the most popular songs in the corpus: Smells Like Teen Spirit by Nirvana and Yellow by Coldplay. The structures are shown with timbre-based self-similarity matrices that are summarized on the bar level. Both are also use euclidean normalization and cosine distance. The combination of these shows the clearest structure. In a self-similarity matrix, every time instance is compared to itself. The blue points and squares show that there is a similarity in timbre.

High levels of similarity can be seen with checkerboard patterns. Those are clearly more profound in Smells Like Teen Spirit, where the overall structure is clearly visible. It roughly has a A-B-C-B-C-C-B-C structure. This structure is a lot less clear in Yellow, meaning the timbre changes throughout the track, or Spotify has trouble picking up on the timbre. By listening to this track, there are clearly repeating patterns, giving me reasons to suspect it is the second reason.  

### Track-Level Features: Tempo and Loudness
```{r track-features-2}
playlist_full |>
  mutate(
    sections =
      map(
        sections,                                   
        summarise_at,
        vars(tempo, loudness, duration),            
        list(section_mean = mean, section_sd = sd)
      )
  ) |>
  unnest(sections) |>
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = artist_name,
      alpha = loudness
    )
  ) +
  geom_point(size = 4) +
  geom_rug() +
  theme_light() +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    title = "Variation in Tempo and Loudness",
    colour = "Band Name",
    alpha = "Loudness (dBFS)"
  )


```

***
Now lets have a look at two other important track-level features: tempo and loudness/volume. The plot on the left shows these features for each individual track in the corpus on the sections level. When looking at the alpha of each track, there does not seem to be a big difference in the loudness between tracks. Only The Beatles do have some tracks with a low volume.

The tempo in this graph yields some interesting results. When it comes to the mean tempoin BPM, Nirvana seems to be overall the highest. Coldplay and The Beatles have around the same median tempo for their tracks. The biggest differences, however, are notable with the variation of different used tempo's throughout each track. Most of the tracks of all three bands have a low variation in tempo. Nirvana has again the highest, followed by The Beatles and then Coldplay. All bands also have noticable outliers, with Coldplay having the most. Let’s have a closer look at some of those outliers in the next chapter. 

### A closer Look at Tempo

```{r tempgrams}
beatles_love <- get_tidy_audio_analysis("6KqiVlOLfDzJViJVAPym1p")
nirvana_marigold <- get_tidy_audio_analysis("40VSbBSYbPN10vJYeZq4tm")

# tempogram_1 <-
#   beatles_love |>
#     tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
#     ggplot(aes(x = time, y = bpm, fill = power)) +
#     geom_raster() +
#     scale_fill_viridis_c(guide = "none") +
#     labs(title = "All You Need is Love - The Beatles",
#         x = "Time (s)",
#         y = "Tempo (BPM)") +
#     theme_classic()
# 
# tempogram_2 <- 
#   nirvana_marigold |>
#     tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
#     ggplot(aes(x = time, y = bpm, fill = power)) +
#     geom_raster() +
#     scale_fill_viridis_c(guide = "none") +
#     labs(title = "Marigold - Nirvana",
#         x = "Time (s)",
#         y = "Tempo (BPM)") +
#     theme_classic()
# 
# plot_grid(tempogram_1, tempogram_2, ncol = 1)
```

***
As seen in the previous chapter, there are a few tracks with high variations of tempo. In this chapter we will have a look at some of these. We will not look at the most expreme outliers, but at those in-between 

One of those outliers is "All You Need Is Love" from The Beatles, with a tempo standard deviation of 7.5. The top plot on the left shows the fourier tempogram from this track. Throughout the track, the tempo seems so slowly build up. There is also a lot of strong tempo-harmonics at play. Especially the at 3:1 the tempo envelope. A point of interest is around the 200 second mark, where for a brief moment tempo estimation becomes really hard

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6KqiVlOLfDzJViJVAPym1p?utm_source=generator&theme=0" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

The second outlier is "Marigold" from Nirvana. It has around the variation in tempo of "All You Need Is Love", but a higher overall tempo. This track also slowly build up the tempo throughout the track. A notable difference here is the lack of strong tempo-harmonics. There is only a strong harmonic at two times the tempo envelope, but it is not as strong as the top graph. There is also a weaker harmonic at around 3:1 the tempo envelope. But with this track, there is no big time periods where tempo estimation gets hard.

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/40VSbBSYbPN10vJYeZq4tm?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### What Features Distinguise The Bands The Most?
```{r classication-pre-processing, include=FALSE}
band_features <-
  playlist_full |>
  mutate(
    playlist = factor(artist_name),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

band_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = band_features
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())

band_cv <- band_features |> vfold_cv(5)
```

```{r feature-ranking}
forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
band_forest <- 
  workflow() |> 
  add_recipe(band_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    band_cv, 
    control = control_resamples(save_pred = TRUE)
  )

workflow() |> 
  add_recipe(band_recipe) |> 
  add_model(forest_model) |> 
  fit(band_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature Ranking in Importance", x = NULL, y = "Importance")
```

***
We have now seen a lot of different visualizations that differentiate the three bands? But what features are the most important differences between the bands? To answer this question, a random forest decision tree was trained. This algorithm gives a ranking of how important each feature is for classification. The ranking for the three bands can be seen on this plot.

According to this ranking, there is a big difference between a few features. One of those is valence, which we have already seen in the first chapter. No big surprise that it ranks high, since we have already seen there is a big difference in valence between the bands. Two other important features that we have not seen before is the duration of the tracks and acousticness.

Two other big differences can be seen in the spectral envelop. This is most notable with the c05 and c06 partials. There does not seem to be a big distinction in the different chroma's used.

### Clustering of Tracks
```{r clustering-pre-processing, include=FALSE}
top_10 <- band_features %>%
  group_by(artist_name) %>%
  top_n(10, track.popularity)

top10_features <- bind_rows(top_10)

playlist_juice <-
  recipe(
    track.name ~
      loudness +
      acousticness +
      instrumentalness +
      valence +
      c02 + c05 + c06 + c11,
    data = top10_features
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(top10_features |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")

playlist_dist <- dist(playlist_juice, method = "euclidean")

data_for_clustering <- playlist_dist |> 
  hclust(method = "average") |> 
  dendro_data() 

playlist_data_for_join <- top10_features %>%
  select(track.name, artist_name) %>%
  mutate(label = str_trunc(track.name, 20))

data_for_clustering$labels <- data_for_clustering$labels %>%
  left_join(playlist_data_for_join)

data_for_clustering$labels$label <- factor(data_for_clustering$labels$label)
```

```{r dendogram}
data_for_clustering |>
  ggdendrogram() +
  geom_text(data = label(data_for_clustering), aes(x, y, 
                                   label=label, 
                                   hjust=0, 
                                   colour=artist_name), size=3) +
  coord_flip() + 
  scale_y_reverse(expand=c(0.2, 0)) +
  theme(axis.line.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank(),
        axis.title.y=element_blank(),
        panel.background=element_rect(fill="white"),
        panel.grid=element_blank()) +
  labs(title = "Band Clustering") +
  guides(
    colour = guide_legend(
      title = "Band"
    )
  )
```
### Conclusion
hallo